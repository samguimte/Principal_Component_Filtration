{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary libraries\n",
    "\n",
    "\n",
    "import opensoundscape\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import librosa\n",
    "import torch\n",
    "import random\n",
    "import sys\n",
    "from PIL import Image\n",
    "from scipy.ndimage import median_filter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image as im \n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "\n",
    "annotations = pd.read_csv(\"../raw_sonobuoy_images/modified_annotations.csv\")\n",
    "\n",
    "unique_annotation = annotations.drop_duplicates(subset=['spectrogram_path'])\n",
    "\n",
    "annotations_modded = annotations.copy()\n",
    "\n",
    "annotations_modded[\"spectrogram_path\"] = annotations[\"spectrogram_path\"].str.replace('raw_sonobuoy_images', 'processed_sonobuoy_images', regex=False)\n",
    "\n",
    "annotations_modded.to_csv(\"../processed_sonobuoy_images/modified_annotations.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spectrogram_path</th>\n",
       "      <th>label</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../raw_sonobuoy_images/CC0407-SB10-040717-2222...</td>\n",
       "      <td>D</td>\n",
       "      <td>406</td>\n",
       "      <td>83</td>\n",
       "      <td>447</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../raw_sonobuoy_images/CC0407-SB10-040717-2222...</td>\n",
       "      <td>D</td>\n",
       "      <td>106</td>\n",
       "      <td>83</td>\n",
       "      <td>146</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../raw_sonobuoy_images/CC0407-SB17-040719-2050...</td>\n",
       "      <td>D</td>\n",
       "      <td>431</td>\n",
       "      <td>67</td>\n",
       "      <td>471</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../raw_sonobuoy_images/CC0407-SB17-040719-2050...</td>\n",
       "      <td>D</td>\n",
       "      <td>131</td>\n",
       "      <td>67</td>\n",
       "      <td>171</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../raw_sonobuoy_images/CC0407-SB2-040713-23500...</td>\n",
       "      <td>D</td>\n",
       "      <td>338</td>\n",
       "      <td>77</td>\n",
       "      <td>372</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    spectrogram_path label  xmin  ymin  xmax  \\\n",
       "0  ../raw_sonobuoy_images/CC0407-SB10-040717-2222...     D   406    83   447   \n",
       "1  ../raw_sonobuoy_images/CC0407-SB10-040717-2222...     D   106    83   146   \n",
       "2  ../raw_sonobuoy_images/CC0407-SB17-040719-2050...     D   431    67   471   \n",
       "3  ../raw_sonobuoy_images/CC0407-SB17-040719-2050...     D   131    67   171   \n",
       "4  ../raw_sonobuoy_images/CC0407-SB2-040713-23500...     D   338    77   372   \n",
       "\n",
       "   ymax  \n",
       "0   123  \n",
       "1   123  \n",
       "2   123  \n",
       "3   123  \n",
       "4   129  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining draw image functions\n",
    "\n",
    "def draw_img(ax, img_vector, h=141, w=601):\n",
    "    \"\"\"\n",
    "    1. takes img_vector,\n",
    "    2. reshapes into right dimensions,\n",
    "    3. draws the resulting image\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ax.imshow( (img_vector).reshape(h,w), cmap=plt.cm.gray)\n",
    "    \n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "def draw_img_single(img_vector, h=141, w=601):\n",
    "    \"\"\"\n",
    "    1. takes img_vector,\n",
    "    2. reshapes into right dimensions,\n",
    "    3. draws the resulting image\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    plt.imshow( (img_vector).reshape(h,w), cmap=plt.cm.gray)\n",
    "    \n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we construct the matrix of image data (rows are spectrograms and columns are pixel values for each spectrogram)\n",
    "\n",
    "data_matrix = []\n",
    "\n",
    "for index, row in unique_annotation.iterrows():\n",
    "\n",
    "    image = Image.open(row['spectrogram_path'])\n",
    "\n",
    "    pixel_values = np.array(list(image.getdata()))\n",
    "\n",
    "    data_matrix.append(pixel_values)\n",
    "\n",
    "stacked_specs = np.vstack(data_matrix)\n",
    "\n",
    "\n",
    "scaler = StandardScaler(with_std=False)\n",
    "data_matrix_mod1 = scaler.fit_transform(stacked_specs)\n",
    "original_data = data_matrix_mod1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Singular Value Decomposition\n",
    "\n",
    "\n",
    "# U is the eigenbasis of the original space (eigen vectors span directions of maximal variance)\n",
    "\n",
    "# S is the diagonal matrix of singular values of \"original_data\" that determine the extent to which the eigen axes are stretched to create the original features; these are the square roots of the eigenvalues for the covariance matrix of the data\n",
    "\n",
    "# T is the transpose of the matrix whose columns represent the principal components of \"original_data\" (these columns are the orthonormal basis vectors of the transformed space that point in the directions of maximal variance)\n",
    "\n",
    "U, S, T = np.linalg.svd(original_data, full_matrices=False)\n",
    "\n",
    "US = U*S\n",
    "\n",
    "svd_data = US @ T\n",
    "\n",
    "svd_data_scaled = scaler.inverse_transform(svd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the cell with commented code on plotting all these spectrograms :3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fig, axs = plt.subplots(100, 2, figsize = (30, 200))\n",
    "\n",
    "#for idx, ax in enumerate(axs.flat):\n",
    "    \n",
    "#    draw_img(ax, matrix[idx])\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the functions to apply our technique on whale calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function serves to calculate the Signal to Noise Ratio of a denoised Spectrogram via the following formula:\n",
    "\n",
    "$$\n",
    "\\text{SNR} = 10 \\log_{10} \\left( \\frac{\\sum_{i,j} I_{\\text{original}}(i,j)^2}{\\sum_{i,j} (I_{\\text{original}}(i,j) - I_{\\text{denoised}}(i,j))^2} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function returns the Signal to Noise Ratio of a given bound adjusted spectrogram's denoised counterpart (this assumes the inputs are the flattened, bounded signal matrices)\n",
    "\n",
    "def SNR(original_signal, denoised_signal):\n",
    "\n",
    "    signal_to_noise_ratio = 10 * math.log10(np.square(original_signal).sum() / np.square(original_signal - denoised_signal).sum())\n",
    "\n",
    "    return signal_to_noise_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the prototype pcf function (no longer in use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The following function returns a tuple containing the reconstruction, and the percentile hyperparameter settings that yield the highest average SNR throughout the processed dataset.\n",
    "\n",
    "\n",
    "# def feature_column_sub(data, feature_percentile = 50, observation_percentile = 50, num_components = 3, variance_explained = None, height = 141, width = 601):\n",
    "\n",
    "#     # Using Singular Value Decomposition to generate the principal eigenvectors\n",
    "\n",
    "#     U, S, T = np.linalg.svd(data, full_matrices=False)\n",
    "\n",
    "#     US = U*S\n",
    "\n",
    "#     # Filtering the specified number of principal components\n",
    "    \n",
    "#     # First check if user wants to specify the number of components or if they will opt to calculate the number of components based on a variance threshold\n",
    "\n",
    "#     if variance_explained == None:\n",
    "\n",
    "#         # Creating a matrix to contain the pixel magnitude adjusted principal components\n",
    "\n",
    "#         signal_enhanced_features = np.zeros_like(T[0:num_components,:])\n",
    "\n",
    "\n",
    "#         # Now let i be the number of rows in the new Principal Component matrix to iterate through each \"eigen - signal\"\n",
    "\n",
    "#         for i in range(len(signal_enhanced_features)):\n",
    "            \n",
    "#             # pick out the ith \"eigen - signal\" as a 2d spectrogram matrix\n",
    "\n",
    "#             feature = np.copy(T[i].reshape((height, width)))\n",
    "\n",
    "#             # Let j be the number of columns in the feature and iterate through every column, extract the user-specified percentile value and subtract this value from the whole column vector; move on to the next column, extract percentile value, subtract, iterate and repeat;\n",
    "#             # note that we set all values that become negative as a result to 0 automatically\n",
    "\n",
    "#             for j in range(feature.shape[1]):\n",
    "#                 column = feature[:, j]\n",
    "#                 percentile_value = np.percentile(column, feature_percentile)\n",
    "#                 feature[:, j] = column - percentile_value\n",
    "#                 feature[:, j][feature[:, j] < 0] = 0\n",
    "\n",
    "#             # Store this noise - adjusted \"eigen - signal\" in the new \"Principal Component\" matrix\n",
    "\n",
    "#             signal_enhanced_features[i] = feature.flatten()\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         # Using the user's threshold for variance explained in the data, we need to calculate how many components will be kept during the PCA algorithm\n",
    "\n",
    "        \n",
    "#         # First create a new whose values correspond to the variance explained for each singular value then create another vector whose entries are the ordered cumulative sum (until ~1) of those variance values\n",
    "\n",
    "#         # Then just add 1 to the index of the entry in this vector whose value is at least that of the user-defined threshold\n",
    "        \n",
    "#         variance_individual = (S ** 2) / np.sum(S ** 2)\n",
    "\n",
    "#         cumulative_variance = np.cumsum(variance_individual)\n",
    "\n",
    "#         num_components = np.argmax(cumulative_variance >= variance_explained) + 1\n",
    "        \n",
    "\n",
    "\n",
    "#         # Same code as above in the \"if\" block just using the new number of components\n",
    "\n",
    "#         signal_enhanced_features = np.zeros_like(T[0:num_components,:])\n",
    "\n",
    "#         for i in range(len(signal_enhanced_features)):\n",
    "\n",
    "#             feature = np.copy(T[i].reshape((height, width)))\n",
    "\n",
    "#             for j in range(feature.shape[1]):\n",
    "#                 column = feature[:, j]\n",
    "#                 percentile_value = np.percentile(column, feature_percentile)\n",
    "#                 feature[:, j] = column - percentile_value\n",
    "#                 feature[:, j][feature[:, j] < 0] = 0\n",
    "\n",
    "#             signal_enhanced_features[i] = feature.flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Finally, we reconstruct the data matrix with our observations using the filtered components and the mixing matrix US (rescaling and adjusting negative values as needed)\n",
    "\n",
    "#     raw_reconstruction = US[:, 0:num_components] @ signal_enhanced_features[0:num_components, :]\n",
    "\n",
    "#     reconstruction_scaled = scaler.inverse_transform(raw_reconstruction)\n",
    "\n",
    "#     reconstruction = np.where(reconstruction_scaled < 0, 0, reconstruction_scaled)\n",
    "\n",
    "\n",
    "#     return reconstruction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the more up to date pcf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def principal_component_filtration(data, num_components = 3, variance_explained = None, height = 141, width = 601):\n",
    "    \n",
    "    # Using Singular Value Decomposition to generate the principal eigenvectors\n",
    "    \n",
    "    U, S, T = np.linalg.svd(data, full_matrices = False)\n",
    "    \n",
    "    US = U*S\n",
    "    \n",
    "    \n",
    "    # Next, initialize an iterable to tune the percentile hyperparameters\n",
    "    \n",
    "    percentiles = [10, 20, 30, 40, 50, 60, 70]\n",
    "    \n",
    "    # Filtering the specified number of principal components\n",
    "    \n",
    "    # First check if user wants to specify the number of components or if they will opt to calculate the number of components based on a variance threshold\n",
    "    \n",
    "    if variance_explained == None:\n",
    "        \n",
    "        # Initialize an empty list that will store \"three-tuples\" (x, y, z) whose first entry represents the data set's average denoised SNR along with the feature and observation percentile hyperparameter values in the 2nd and 3rd entries respectfully\n",
    "        \n",
    "        snr_values = []\n",
    "        \n",
    "        # Iterate through each combination of feature percentile and observation percentile and calculate the associated average SNR across the resulting data set reconstruction\n",
    "        \n",
    "        for feature_percentile in percentiles:\n",
    "            \n",
    "            for observation_percentile in percentiles:\n",
    "                \n",
    "                # Creating a matrix to contain the pixel magnitude adjusted principal components\n",
    "                \n",
    "                signal_enhanced_features = np.zeros_like(T[0:num_components,:])\n",
    "                \n",
    "                # Now let i be the number of rows in the new Principal Component matrix to iterate through each \"eigen - signal\"\n",
    "                \n",
    "                for i in range(len(signal_enhanced_features)):\n",
    "                    \n",
    "                    # pick out the ith \"eigen - signal\" as a 2d spectrogram matrix\n",
    "                    \n",
    "                    feature = np.copy(T[i].reshape((height, width)))\n",
    "                    \n",
    "                    # Let j be the number of columns in the feature and iterate through every column, extract the user-specified percentile value and subtract this value from the whole column vector; move on to the next column, extract percentile value, subtract, iterate and repeat;\n",
    "                    # note that we set all values that become negative as a result to 0 automatically\n",
    "                    \n",
    "                    for j in range(feature.shape[1]):\n",
    "                        column = feature[:,j]\n",
    "                        percentile_value = np.percentile(column, feature_percentile)\n",
    "                        feature[:, j] = column - percentile_value\n",
    "                        feature[:, j][feature[:, j] < 0] = 0\n",
    "                        \n",
    "                    \n",
    "                    # Store this noise - adjusted \"eigen - signal\" in the new \"Principal Component\" matrix\n",
    "                    \n",
    "                    signal_enhanced_features[i] = feature.flatten()\n",
    "                    \n",
    "                \n",
    "                # reconstructing the data matrix and rescaling (as well as ensuring no negative values remain)\n",
    "                \n",
    "                raw_reconstruction = US[:, 0:num_components] @ signal_enhanced_features[0:num_components, :]\n",
    "                \n",
    "                reconstruction_scaled = scaler.inverse_transform(raw_reconstruction)\n",
    "                \n",
    "                reconstruction = np.where(reconstruction_scaled < 0, 0, reconstruction_scaled)\n",
    "                \n",
    "                # applying backward time-wise subtraction to the more uniform-noise distributed data set\n",
    "                \n",
    "                for h in range(len(reconstruction)):\n",
    "                    \n",
    "                    spectrogram = reconstruction[h].reshape((height, width))\n",
    "                    \n",
    "                    for g in range(spectrogram.shape[1]):\n",
    "                        column = spectrogram[:,g]\n",
    "                        percentile_value = np.percentile(column, observation_percentile)\n",
    "                        spectrogram[:,g] = column - percentile_value\n",
    "                        spectrogram[:,g][spectrogram[:,g] < 0] = 0\n",
    "                        \n",
    "                    reconstruction[h] = spectrogram.flatten()\n",
    "                    \n",
    "                \n",
    "                # initializing a temporary numpy array to store the SNR for each spectrogram observation\n",
    "                \n",
    "                temporary_snr_array = np.array([])\n",
    "                \n",
    "                # iterating through each observation, calculating the SNR, then storing in the temporary array\n",
    "                \n",
    "                for i in range(len(reconstruction)):\n",
    "                    \n",
    "                    temporary_snr_array = np.append(temporary_snr_array, SNR(data[i], reconstruction[i]))\n",
    "                    \n",
    "                \n",
    "                # finally, we simply store the average SNR for the entire data set in the \"snr_values\" list\n",
    "                \n",
    "                snr_values.append((np.mean(temporary_snr_array), feature_percentile, observation_percentile))\n",
    "                \n",
    "                \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        # Using the user's threshold for variance explained in the data, we need to calculate how many components will be kept during the PCA algorithm\n",
    "        \n",
    "        # First create a new whose values correspond to the variance explained for each singular value then create another vector whose entries are the ordered cumulative sum (until ~1) of those variance values\n",
    "        \n",
    "        # Then just add 1 to the index of the entry in this vector whose value is at least that of the user-defined threshold\n",
    "        \n",
    "        variance_individual = (S ** 2) / np.sum(S ** 2)\n",
    "        \n",
    "        cumulative_variance = np.cumsum(variance_individual)\n",
    "        \n",
    "        num_components = np.argmax(cumulative_variance >= variance_explained) + 1\n",
    "        \n",
    "        \n",
    "        # The steps taken here are identical to those taken in the \"if\" block for this algorithm\n",
    "        \n",
    "        snr_values = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        for feature_percentile in percentiles:\n",
    "            \n",
    "            for observation_percentile in percentiles:\n",
    "                \n",
    "                # Creating a matrix to contain the pixel magnitude adjusted principal components\n",
    "                \n",
    "                signal_enhanced_features = np.zeros_like(T[0:num_components,:])\n",
    "                \n",
    "                # Now let i be the number of rows in the new Principal Component matrix to iterate through each \"eigen - signal\"\n",
    "                \n",
    "                for i in range(len(signal_enhanced_features)):\n",
    "                    \n",
    "                    # pick out the ith \"eigen - signal\" as a 2d spectrogram matrix\n",
    "                    \n",
    "                    feature = np.copy(T[i].reshape((height, width)))\n",
    "                    \n",
    "                    # Let j be the number of columns in the feature and iterate through every column, extract the user-specified percentile value and subtract this value from the whole column vector; move on to the next column, extract percentile value, subtract, iterate and repeat;\n",
    "                    # note that we set all values that become negative as a result to 0 automatically\n",
    "                    \n",
    "                    for j in range(feature.shape[1]):\n",
    "                        column = feature[:,j]\n",
    "                        percentile_value = np.percentile(column, feature_percentile)\n",
    "                        feature[:, j] = column - percentile_value\n",
    "                        feature[:, j][feature[:, j] < 0] = 0\n",
    "                        \n",
    "                    \n",
    "                    # Store this noise - adjusted \"eigen - signal\" in the new \"Principal Component\" matrix\n",
    "                    \n",
    "                    signal_enhanced_features[i] = feature.flatten()\n",
    "                    \n",
    "                \n",
    "                raw_reconstruction = US[:, 0:num_components] @ signal_enhanced_features[0:num_components, :]\n",
    "                \n",
    "                reconstruction_scaled = scaler.inverse_transform(raw_reconstruction)\n",
    "                \n",
    "                reconstruction = np.where(reconstruction_scaled < 0, 0, reconstruction_scaled)\n",
    "                \n",
    "                for h in range(len(reconstruction)):\n",
    "                    \n",
    "                    spectrogram = reconstruction[h].reshape((height, width))\n",
    "                    \n",
    "                    for g in range(spectrogram.shape[1]):\n",
    "                        column = spectrogram[:,g]\n",
    "                        percentile_value = np.percentile(column, observation_percentile)\n",
    "                        spectrogram[:,g] = column - percentile_value\n",
    "                        spectrogram[:,g][spectrogram[:,g] < 0] = 0\n",
    "                        \n",
    "                    reconstruction[h] = spectrogram.flatten()\n",
    "                    \n",
    "                \n",
    "                temporary_snr_array = np.array([])\n",
    "                \n",
    "                for i in range(len(reconstruction)):\n",
    "                    \n",
    "                    temporary_snr_array = np.append(temporary_snr_array, SNR(data[i], reconstruction[i]))\n",
    "                    \n",
    "                \n",
    "                snr_values.append((np.mean(temporary_snr_array), feature_percentile, observation_percentile))\n",
    "                \n",
    "                \n",
    "                \n",
    "    \n",
    "    \n",
    "    max_snr_settings = max(snr_values, key=lambda x: x[0])\n",
    "    \n",
    "    return max_snr_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The first application will be on Michaela's Data Set\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
